Partial reproduction and alternative analysis of "Comparing the Biological Impact of Glatiramer Acetate with the Biological Impact of a Generic"
========================================================

`r as.character(Sys.time())`
<br/>
<br/>

### Overview

This report aims to show that the use of the statistical tool ComBat from [Johnson et al.](http://biostatistics.oxfordjournals.org/content/8/1/118.abstract) led to false results in the analysis performed in [Towfic et al.'s ](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0083757) "Comparing the Biological Impact of Glatiramer Acetate with the Biological Impact of a Generic".

The generation of this report is a side product of  "Methods that remove batch effects while retaining group differences may lead
to exaggerated confidence in downstream analyses", V. Nygaard, E. A. Rødland, E. Hovig, manuscript in preparation. It consists of some of the analyses performed working with the manuscript that were too detailed to be included. Nevertheless, it should be useful for those especially interested. This report is not peer-reviewed.

The format of this report is html with text, r-code and plots intermingled and made from an rmarkdown file in R-studio with knitr. It is intended to be reproducible if run in conjunction with a few accompanying files from the github repository (https://github.com/ous-uio-bioinfo-core/batch-adjust-warning-reports.git).

This document has five main parts

- Getting and formatting the data
- Reproducing some of the results to show that we are working on the same data and analysis workflow
- Remove the use of ComBat and perform a similar analysis with an alternative established tool
- Estimate the error introduced when ComBat is used, and evaluate the consequences for the conclusion of the study
- Perform a few more sanity checks to substantiate that the difference in results for the two analyses above is mainly introduced by ComBat


```{r runparameters, results='hide', echo=FALSE}
starttime = Sys.time()
debug = FALSE
downloaddata=FALSE
set.seed(100)
```

<br/>
<br/>

```{r, results='hide', message=FALSE}
includelibs = c("sva", "limma", "xtable")
tmp=lapply(includelibs, require, character.only=T)
if(any(!unlist(tmp)))
{
  stop( paste("Not able to find all packages. Please install ",
              paste(includelibs[!unlist(tmp)], collapse=", ") )
              )
  #source("http://bioconductor.org/biocLite.R")
  #biocLite(includelibs)
}
source("../../commonscripts/helperfunctions.r")
rm(tmp)
```

The above libraries and helper script files are needed in order to reproduce this report.
<br/>
<br/>

### Getting the data

The important sample information is described in Table S1 from Towfic et al. and its usage in ComBat is described briefly in the methods;

>  Each microarray’s chip designation was supplied a batch label; there were 18 batches in all. The labels for the treatments (i.e. drug product, reference standard…) were added as covariates.<cite> Towfic et al.

Table S1 does have a "Chip"- column, unfortunately there is no dedicated "treatment"-column.
Communication with the corresponding author yielded this explanation

> ..and the only thing we used as a covariate was the unique treatment names themselves (e.g. "Medium"" or "RS").<cite> Towfic et al.

Based on these descriptions and the annotation for the GEO deposit, we compiled a more complete sample annotation file (sampleannotation.csv).

```{r results="asis"}
sampleannotation = read.table("data/sampleannotation.csv", sep="\t", header=TRUE,  stringsAsFactors=FALSE)
sampleannotation$code = make.names(sampleannotation$code)
sampleannotation$chip = as.character(sampleannotation$chip)
dimnames(sampleannotation)[[1]] = sampleannotation$code

print.xtable(xtable ( head(sampleannotation), caption="Head of sampleannotation"), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)
```

<br/>

The "covariate"-column is made based on the "code"-column, but might not match 100% to what was actually used for all samples. The last two columns are from the GEO deposit and reveal the samples that do not have data, which we will remove from the data shortly.

The naming convention seems to differ slightly between the Table S1 and the text in Towfic et al. This is our interpretation of the main covariate labels and its corresponding name in the text.

- **DP** referred to as GA (but not GA as in table S1)
- **N** referred to as "generic"
- **M** referred to as "medium"
- **RS** referred to as "reference standard""

```{r}
# take out 3 samples that are not assigned to a geoaccession. Failed QC?
sampleannotation = sampleannotation[!is.na(sampleannotation$geoaccession),] 
```

The batch/covariate design shows many batches and covariate groups.

```{r results='asis'}
print.xtable(xtable ( table(sampleannotation[, c("chip", "covariate")]), caption="Number of samples per batch/covariate combo. Chip barcode in rows, covariate in columns"), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)
```

<br/>
<br/>

A look at the primary comparison, "DP"(GA) and "N"(generic) reveals a lack of balance.

```{r results='asis'}

print.xtable(xtable ( table(sampleannotation[sampleannotation$covariate %in% c("DP", "N"),
										c("chip", "covariate")]),), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)
```

<br/>
<br/>

> The microarray data have been deposited in the Gene Expression Omnibus, under accession number [GSE40566](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE40566). <cite> Towfic et al.

This was not used in here, but instead the file FORGEO_master_matrix_PLOS_ONE_response.csv was used (a tab separated text version of the excel file).

A recap of the important input files used in this report:

- **sampleannotation.csv** holds the important sample/batch/covariate assignment. Compiled based on information in Towfic et al, the GEO deposit and personal communication.
- **FORGEO_master_matrix_PLOS_ONE_response.csv** the measurements in a sample vs probe matrix. From the [GEO deposit](http://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE40566&format=file&file=GSE40566%5Fnon%5Fnormalized%2Etxt%2Egz). Not in github.


Reading the data.
```{r}
alt_annot = read.table("not_in_github/FORGEO_master_matrix_PLOS_ONE_response.csv", sep="\t", header=FALSE, stringsAsFactors=FALSE, fill=TRUE, nrows=6)
alt_annot=t(alt_annot[,c(-1,-2)])
colnames(alt_annot) = alt_annot[1,]
alt_annot=alt_annot[-1,]
alt_annot=as.data.frame(alt_annot, stringsAsFactors=FALSE)
#need to split arrayid slot and the last number 1or2 which probably is subarray.
tmp = data.frame(do.call('rbind', strsplit(alt_annot[, "Array Address ID"],'_')), stringsAsFactors=FALSE)
alt_annot$slot=tmp[,2]
alt_annot$subarray=tmp[,3]

alt_data=read.table("not_in_github/FORGEO_master_matrix_PLOS_ONE_response.csv", sep="\t", header=TRUE, stringsAsFactors=FALSE, fill=TRUE, skip=5)
rownames(alt_data)=alt_data[,2]
alt_data = alt_data[,c(-1,-2,-3)]
alt_data = as.matrix(alt_data)

#combine the two subarray coloumns
datamatrix_raw = avearrays(alt_data, paste(alt_annot$Chip, alt_annot$slot, sep="_") )

#sort as in the sampleannotation.
a =match( paste(sampleannotation$chip, sampleannotation$slot, sep="_"),
			 colnames(datamatrix_raw))
datamatrix_raw = datamatrix_raw[,a]
rm(a)

```


Several tables with results are presented in the Supporting Information in Towfic et al. We are aiming to reproduce two of these. Unfortunately, they were only presented in a pdf-format not easily parsed by a computer. We had to resort to an ad hoc method of cut-and-paste from pdf into text files which were somewhat polluted by pdf-formatting code. For some tables, about 10% of the rows will be lost, but the rest will suffice for our purpose.

```{r}
# Table S2
table_s2 = read.table("data/table_s2.csv", sep="\t", header=TRUE, stringsAsFactors=FALSE, fill=TRUE)
table_s2 = as.matrix(table_s2)
table_s2[,3:6] = as.numeric(table_s2[,3:6])
# not able to paste the pdf whitout a lot of gibberish clutter the data. Some probes are lost!
a =rowSums(is.na(table_s2[,3:6])) > 0
print(paste("Lost rows table s2: ", sum(a)))
table_s2=data.frame(table_s2[!a,], stringsAsFactors=FALSE)
# data.frame makes the columns characters again.
for(n in 3:6)
{
  table_s2[,n]=as.numeric(table_s2[,n])
}

# Table S5
table_s5 = read.table("data/table_s5.csv", sep="\t", header=TRUE, stringsAsFactors=FALSE, fill=TRUE)
tmp = as.matrix(table_s5)
tmp[,3:14] = as.numeric(tmp[,3:14])
# not able to paste the pdf whitout a lot of gibberish clutter the data. Some probes are lost!
a =rowSums(is.na(tmp[,3:14])) > 0
print(paste("Lost rows table s5: ", sum(a)))
table_s5=data.frame(table_s5[!a,], stringsAsFactors=FALSE)
table_s5$Fold_Change = as.numeric(table_s5$Fold_Change)

```
<br/>
<br/>


### Reproduce some of the original results

Towfic et al. performed several different tests for these data, and it is outside the scope of this report to reproduce all of their results. We focus on the key part of testing for differentially expressed genes between "GA" (DP) and "generic"(N) as described in Table S2 and Table S5. But prior to those tests, we have to preprocess according to the description.

> Starting with background-corrected bead-level signals, we quantile normalized the extracted data for all samples across all 46,547 probes via the “preprocessCore” package in R. <cite> Towfic et al.

```{r}
datamatrices = list()
datamatrices[["real_raw"]] = datamatrix_raw
if(debug)
{
  datamatrices[["real_raw"]] = datamatrix_raw[1:1000,]
  table_s2 = table_s2[table_s2$ID %in% rownames(datamatrices[["real_raw"]]),]
  table_s5 = table_s5[table_s5$Probe %in% rownames(datamatrices[["real_raw"]]), ]
}
datamatrices[["real_qnorm"]] = normalizeBetweenArrays(datamatrices[["real_raw"]], method="quantile")
# normalize.quantiles from the package preprocessCore is used in the paper, but seems to do the same as the limma version.
```

> We then corrected for batch variation with ComBat [17] as implemented in the SVA package of R [18]. Each microarray’s chip designation was supplied a batch label; there were 18 batches in all. The labels for the treatments (i.e. drug product, reference standard…) were added as covariates. <cite> Towfic et al.

```{r}
combatmod = model.matrix(~as.factor(sampleannotation$covariate))
datamatrices[["real_combat_covariates"]]= as.matrix(ComBat(dat=datamatrices[["real_qnorm"]],
                                                           batch=sampleannotation$chip,
                                                           mod=combatmod,
                                                           numCovs=NULL,
                                                           par.prior=TRUE,
                                                           prior.plots=FALSE))

```

<br/>
<br/>

**Table S2**

First we will try to reproduce Table S2.

> Genes utilized for the tolerance method illustrated in Figure 1B. <cite> Towfic et al.(figure text)

> This standard of comparison was constructed by first identifying the top 1000 probes by absolute fold change of reference standard compared to the medium (Table S2). The list includes both upregulated and downregulated probes compared to medium. Probes were filtered such that ones upregulated by reference standard needed to have an average reference standard expression of 6.00 or higher and ones downregulated by reference standard needed to have an average medium expression of 6.00 or higher. <cite> Towfic et al.

This is how the table looks

```{r results='asis'}


print.xtable(xtable ( head(table_s2), caption="Head of Table S2"), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)

```

```{r}
thiscovariatename = "covariate"
thisdata = datamatrices[["real_combat_covariates"]]

# the mean uses log2 numbers and will be a geometric mean.
meanM = rowMeans(thisdata[, sampleannotation[, "covariate"]=="M"]) 
meanRS = rowMeans(thisdata[, sampleannotation[, "covariate"]=="RS"]) 
meanDP = rowMeans(thisdata[, sampleannotation[, "covariate"]=="DP"])
meanN = rowMeans(thisdata[, sampleannotation[, "covariate"] =="N"]) 
rm(thisdata)
foldchange_RS_vs_M = meanRS - meanM
foldchange_RS_vs_M = foldchange_RS_vs_M[meanM>6 | meanRS>6]
top1000_RS_vs_M = names(foldchange_RS_vs_M[order(abs(foldchange_RS_vs_M), decreasing=TRUE)])[1:1000]
table(table_s2$ID %in% top1000_RS_vs_M)
```

Out of Table S2's **`r nrow(table_s2)`** probes, **`r sum(table_s2$ID %in% top1000_RS_vs_M)`** were reproduced. Scatter plots of the actual values show a correlation.

```{r reproduced_tables2, dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}
# scatter plot correlation of the 4 columns in table_S2 vs. the reproduced numbers.
par(mfrow=c(2, 2))
plot(table_s2$AVG_Medium, meanM[table_s2$ID], pch=".")
plot(table_s2$AVG_Reference_Standard, meanRS[table_s2$ID], pch=".")
plot(table_s2$AVG_GA, meanDP[table_s2$ID], pch=".")
plot(table_s2$AVG_generic, meanN[table_s2$ID], pch=".")
```

Original values from table S2 on X-axis, reproduced values on Y-axis. The values in Table S2 is almost reproduced.

<br/>
<br/>

**Table S5**

This is how the table looks. It is transposed and shows only the top 3 genes for visualization purposes.

```{r results="asis"}

print.xtable(xtable (t(table_s5[1:3,]), caption="top of Table_S5"), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)

```

<br/>
<br/>

> Comparison of expression in GA to expression in GA for each probe, including fold change, ANOVA, LIMMA with background subtraction, comparative marker selection by signal-to-noise ratio, comparative marker selection by t-test, and the Wilcoxon non-parametric method. <cite> Figure text, Towfic et al.

There must be a typo in the figure text, they meant "Comparison of expression in GA to expression in **generic** for each probe, ...".

> To find differentially expressed probes between generic and GA, we utilized various statistical tests at the probe level and merged the results across the different methods. <cite> Towfic et al.

How the fold change was calculated is not explained. A common way is to take the difference of geometric means that were calculated for Table S2 above.

```{r}
foldchange_DP_N = meanDP - meanN
```

These reproduced fold changes can be compared to the probes listed in table S5.

```{r dev='png', fig.width=10, fig.height=5, fig.show='hold', tidy=FALSE}
par(mfrow=c(1, 2))
plot(table_s5$Fold_Change, foldchange_DP_N[table_s5$Probe],
     pch=".", xlab="Original Table S5", ylab="Reproduced",
     main="FC comparison")

thiscolors = c("black", "red")
hist(foldchange_DP_N, breaks=100, freq=F, border=thiscolors[1], main="Reproduced FC for DP vs. N")
hist(foldchange_DP_N[table_s5$Probe], breaks=100, add=T, freq=F, border=thiscolors[2])
legend("topright", legend=c("All probes", "Table S5 probes"), text.col=thiscolors)
```

The first plot reveals that the reproduced fold changes are much more compressed than the ones in Table S5.
There seems to be two linear relationship, one for negative FC's and another for positive FC's. They seem to have different intercepts, but comparable slope. The second plot shows that the reproduced fold changes for the probes listed in Table S5 are clearly more extreme than the rest.  

The fold changes in table 5S do not match the fold changes computed from the average expression values listed in table S2 either, as a plot will show;

```{r dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}
# table S2 has the AVG for both DP and N listed
fc_table_s2 = table_s2$AVG_GA - table_s2$AVG_generic
names(fc_table_s2) = table_s2$ID
x = table_s5$Fold_Change
y = fc_table_s2[table_s5$Probe]
lim =c( min(   c(min(x, na.rm=T), min(y, na.rm=T))  ), max( c(max(x, na.rm=T), max(y, na.rm=T)) ))
plot(x, y , ylim=lim, xlim=lim, pch="." ,  xlab="Table_S5", ylab="Table_S2",
     main="FC comparison, given values.") #  almost perfect match with offset.
rm(x, y, lim)
```

There seems to be some inconsistency between the fold changes in table S5 and the reproduced ones. However, it is the confidence i.e p-values that are affected by the use of ComBat. 

The rest of table S5 are p-values, and we will try to reproduce the ones from limma.

> Next, we utilized Linear Models for Microarray (LIMMA) data analysis [23], [24] R package, part of the Bioconductor framework [25], to compare generic and GA samples, fitting a linear model that adjusts for fixed effect from medium (Effect = (GA-generic) – (generic-Medium)). The coefficients for the linear model were tested for significance using a modified t-test (taking into account standard deviation)...<cite> Towfic et al.

The formula "Effect = (GA-generic) - (generic-Medium)" seems strange.

```{r dev='png', fig.width=10, fig.height=6, fig.show='hold', tidy=FALSE}
group = factor(sampleannotation$covariate)
design = model.matrix(~0 + group)
fit = lmFit(datamatrices[["real_combat_covariates"]], design)
cont.matrix = makeContrasts ( contrasts="(groupDP-groupN) - (groupN-groupM)", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_ret_org = eBayes(fit2)
limma_p_org = limma_ret_org$p.value
#names(limma_p_org) = names(limma_ret_org$Amean)
par(mfrow=c(1, 2))
plot(table_s5[,"limma_p"], limma_p_org[table_s5$Probe,1], xlab="Original Table S5", 
     ylab="Reproduced", main="LIMMA P-values from table S5 vs. reproduced")
thiscolors = c("black", "red")
hist(limma_p_org, breaks=100, freq=T, border=thiscolors[1], main="Reproduced LIMMA p-values for DP vs. N")
hist(limma_p_org[table_s5$Probe,1], breaks=100, add=T, freq=T, border=thiscolors[2])
legend("topright", legend=c("All probes", "Table S5 probes"), text.col=thiscolors)
```

A lot of the reproduced p-values are high for the probes in Table S5. A more straightforward contrast of Effect = GA-generic gives more similar p-values as in Table S5. We assume "GA-generic" or equivalent was used by Towfic et al., and for the remainder of this report, this is what will be used.

```{r dev='png', fig.width=10, fig.height=6, fig.show='hold', tidy=FALSE}
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_ret_alt = eBayes(fit2)
limma_p_alt = limma_ret_alt$p.value
#names(limma_p_alt) = names(limma_ret_alt$Amean)
par(mfrow=c(1, 2))
plot(table_s5[,"limma_p"], limma_p_alt[table_s5$Probe,1], xlab="Original Table S5", 
     ylab="Reproduced", main="LIMMA P-values from table S5 vs. reproduced")
thiscolors = c("black", "red")
hist(limma_p_alt, breaks=100, freq=T, border=thiscolors[1], main="Reproduced LIMMA p-values for DP vs. N")
hist(limma_p_alt[table_s5$Probe,1], breaks=100, add=T, freq=T, border=thiscolors[2])
legend("topright", legend=c("All probes", "Table S5 probes"), text.col=thiscolors)
```

The reproduced p-values for the Table S5 probes are not exactly as the listed ones, but still mostly low. In addition to the limma test, Towfic et al. performed similar tests for significant probes between generic and GA using several other methods. We tried several of them as well, but have skipped them in this report, since they showed a similar pattern as for the limma p-values.

This concludes the reproduction. We managed to reproduce the average expressions listed in Table S2 pretty well. The p-values in Table S5 were with some reasonable interpretation reproduced. The results can now be compared to the same analysis using an alternative to ComBat.

<br/>
<br/>

### Analysis without ComBat, and consequences for the results

Table S2 has average expression values for the conditions, but includes no values about how much trust we can have in those average expressions (i.e significance tests, confidence intervals etc.).

Table S5, on the other hand, consists of results from significance tests that are strongly influenced by ComBat's data transformation. An alternative way of handling the batch effect compared to ComBat's is to include it in the statistical test. This can be done in limma in two ways. First what is described as "Blocking" in the limma user guide.

```{r dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}
group = factor(sampleannotation$covariate)
block = factor(sampleannotation$chip)
design = model.matrix(~0+group+block)
fit = lmFit(datamatrices[["real_qnorm"]], design)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
#limma_ret_woc = eBayes(fit2)
#limma_p_woc = limma_ret_woc$p.value
#names(limma_p_woc) = names(limma_ret_woc$Amean)
limma_p_woc =  eBayes(fit2)$p.value
rm(design, group, block, fit, cont.matrix, fit2)
table(p.adjust(limma_p_alt , method="fdr")<0.05, p.adjust(limma_p_woc , method="fdr")<0.05, dnn=c("ComBat adj", "LIMMA adj"))
```

The number of significant probes using the given FDR threshold of 0.05 when batch effect is handled by LIMMA is 
**`r sum(p.adjust(limma_p_woc , method="fdr")<0.05)`**. This is much lower compared to when ComBat is applied (
**`r sum(p.adjust(limma_p_alt , method="fdr")<0.05)`** ).  The p-value distribution is still skewed, but not as much.

Alternatively, limma´s <i>duplicateCorrelation</i> function can be used to treat batch as a random effect. This could arguably be a more powerful approach, and a short investigation is warranted.

```{r dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}

group = factor(sampleannotation$covariate)
block = factor(sampleannotation$chip)
design = model.matrix(~0+group)

corfit <- duplicateCorrelation(datamatrices[["real_qnorm"]],design,block=block)
fit <- lmFit(datamatrices[["real_qnorm"]],design,block=block,correlation=corfit$consensus)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
#limma_ret_woc = eBayes(fit2)
limma_p_mixed = eBayes(fit2)$p.value

thiscolors = c("red", "blue", "black")
hist(limma_p_alt, breaks=100, freq=T, border=thiscolors[1], main="Alternative P-value distributions for DP vs. N", xlab="P-values")
hist(limma_p_woc, breaks=100, add=T, freq=T, border=thiscolors[2])
hist(limma_p_mixed, breaks=100, add=T, freq=T, border=thiscolors[3])
legend("topright", legend=c("ComBat adjusted", "batch handled by LIMMA as factor", "batch handled by LIMMA as random effect"), text.col=thiscolors)
```

For this data set, the two LIMMA methods will give quite similar results, at least compared to the ComBat adjusted approach. We will only use the first, "Blocking", in subsequent comparisons.

<br/>
<br/>

### Additional sanity checks

To substantiate that the result from the use of ComBat is less trustworthy than the alternative analysis, we provide a couple of additional sanity checks.

<br/>
<br/>

**Random data**

First, we use random numbers drawn from the same distribution(mean=0, sd=1) regardless of batch or covariate, while retaining the batch/covariate design.

```{r}
set.seed(100)
datamatrices[["random_raw"]] = matrix(rnorm(length(datamatrices[["real_raw"]]), mean=0, sd=1), 
                                      nrow=nrow(datamatrices[["real_raw"]]), 
                                      ncol=ncol(datamatrices[["real_raw"]]), 
                                      dimnames=dimnames(datamatrices[["real_raw"]]))
datamatrices[["random_combat_covariates"]]= as.matrix(ComBat(dat=datamatrices[["random_raw"]],
                                                             batch=sampleannotation$chip, 
                                                             mod=combatmod, 
                                                             numCovs=NULL, 
                                                             par.prior=TRUE, 
                                                             prior.plots=FALSE))
```

limma is then used in 3 ways

- On the ComBat adjusted random numbers
- On the random numbers ignoring batch information
- On the random numbers including batch as a blocking factor


```{r}
group = factor(sampleannotation$covariate)
design = model.matrix(~0 + group)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)


fit = lmFit(datamatrices[["random_combat_covariates"]], design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_p_rand_combat = eBayes(fit2)$p.value[,1]

fit = lmFit(datamatrices[["random_raw"]], design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_p_rand_nocombat = eBayes(fit2)$p.value[,1]


block = as.factor(sampleannotation$chip)
design = model.matrix(~0+group+block)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)
fit = lmFit(datamatrices[["random_raw"]], design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_p_rand_batchblocked = eBayes(fit2)$p.value[,1]

```

```{r dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}
par(mfrow=c(1, 1))
thiscolors = c("red", "blue", "black")
hist(limma_p_rand_combat, breaks=100, freq=T, border=thiscolors[1], main="P-values, Random numbers for DP vs. N")
hist(limma_p_rand_nocombat, breaks=100, add=T, freq=T, border=thiscolors[2])
hist(limma_p_rand_batchblocked, breaks=100, add=T, freq=T, border=thiscolors[3])
legend("topright", legend=c("ComBat adjusted", "batch handled by Limma as factor", "No adjustment"), text.col=thiscolors)
```

The p-value plots from the ComBat adjusted random data shows a skewed distribution, although not as much as for the real data. The random data has no batch effect or other effects like the real data which might affect the ComBat adjustment.

<br/>
<br/>

<br/>
<br/>

### References


Johnson, WE, Rabinovic, A, and Li, C (2007). Adjusting batch effects in microarray expression data using Empirical Bayes methods. Biostatistics 8(1):118-127.

Leek JT, Johnson WE, Parker HS, Jaffe AE, Storey JD.(2012) The sva package for removing batch effects and other unwanted variation in high-throughput experiments. Bioinformatics. 2012 Mar 15;28(6):882-3.

Smyth, GK (2005). Limma: linear models for microarray data. In: 'Bioinformatics and Computational Biology Solutions
  using R and Bioconductor'. R. Gentleman, V. Carey, S. Dudoit, R. Irizarry, W. Huber (eds), Springer, New York, pages
  397-420.
  
  R Core Team (2013). R: A language and environment for statistical computing. R Foundation for Statistical Computing,
  Vienna, Austria. URL http://www.R-project.org/

  Yihui Xie (2013). knitr: A general-purpose package for dynamic report generation in R. R package version 1.5.

  Yihui Xie (2013) Dynamic Documents with R and knitr. Chapman and Hall/CRC. ISBN 978-1482203530

  Yihui Xie (2013) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and
  Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595
  
  RStudio Team (2012). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA URL http://www.rstudio.com/.


```{r sessionInfo, comment=""}
sessionInfo()
```

generation ended `r as.character(Sys.time())`. Time spent `r  as.integer(round(difftime(Sys.time(),starttime, units="mins")))` minutes .


