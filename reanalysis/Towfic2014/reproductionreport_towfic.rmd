Partial reproduction and alternative analysis of "Comparing the Biological Impact of Glatiramer Acetate with the Biological Impact of a Generic"
========================================================

`r as.character(Sys.time())`
<br/>
<br/>

### Overview

This report aims to show that the use of the statistical tool ComBat [Johnson et al.](http://biostatistics.oxfordjournals.org/content/8/1/118.abstract) would lead to false results for the analysis described in [Towfic et al.'s ](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0083757) "Comparing the Biological Impact of Glatiramer Acetate with the Biological Impact of a Generic".

The generation of this report is a side product of  "Methods that remove batch effects while retaining group differences may lead
to exaggerated confidence in downstream analyses", V. Nygaard, E. A. Rødland, E. Hovig, manuscript in preparation. It consists of some of the analyses performed working with the manuscript that were too detailed to be included. Nevertheless, it should be useful for those especially interested. This report is not peer-reviewed, and it might be updated.

The format of this report is html with text, r-code and plots intermingled and made from an rmarkdown file in R-studio with knitr. It is intended to be reproducible if run in conjunction with a few accompanying files from the github repository (https://github.com/ous-uio-bioinfo-core/batch-adjust-warning-reports.git).

This document has five main parts

- Deviation from the analysis that were actually performed by Towfic et al
- Getting and formatting the data
- Reproducing some of the results to show that we are working on the same data and analysis workflow
- Remove the use of ComBat and perform a similar analysis with an alternative established tool
- Estimate the error introduced when ComBat is used, and evaluate the consequences for the conclusion of the study
- Perform a few more sanity checks to substantiate that the difference in results for the two analyses above is mainly introduced by ComBat

<br/>
<br/>

### Deviation from the analysis that were actually performed by Towfic et al.

In our submitted manuscript, we used the GEO deposit GSE40566 as a starting point.

> The microarray data have been deposited in the Gene Expression Omnibus, under accession number [GSE40566](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE40566). <cite> Towfic et al.

Through communication with the authors we later understood, that this was not the version of the data used as a starting point in Towfic et al. The data actually used in Towfic et al. is found in GEO deposit with accession GSE61901. These are based on the same hybridizations, but at low-level processed in a different way. We were also given the scripts used to analyse the data in Towfic et al (personal communication, Ben Zeskind). There were some important deviations between our reproduction and Towfic et al.´s analysis, mainly that the data matrix from GSE61901 has two columns per sample, each originating from the same hybridization, but from two separate strips consisting of the same probes.  A sample is flowing freely between the two strips, thus they can be considered as being sub-arrays. In the standard preprocessing done with Illumina's Genome Studio, only one probe value is calculated per hybridization, using information from both strips. And this is how the data is formatted in the GEO deposit (GSE40566) originally linked to from Towfic et al. As long as the data from the two strips are handled as technical replicates, the result from analyses using the two different GEO accession, would be quite similar. However, Towfic et al. did not combine the replicates or use a model that handled technical replicates, instead treating these as independent samples. The consequence of inflating the sample size like this is exaggerated confidence, quite similar to the adverse effects of ComBat. 
<BR/>
<BR/>
We chose Towfic et al. as an illustration of adverse effects of ComBat based on the "Methods" description, which did not describe the presence of technical replicates. To include the inappropriate technical replicate handling in our re-analysis would result in hundreds of false positives, even without the use of ComBat. This might confuse the readers, and the example would be less illustrative of the adverse effects of ComBat. Thus, we decided to leave it out in our article, and analyse the data in a more standard way using the GSE40566 as a starting point. However, in this report, we use the GSE61901 deposit in order to replicate exactly some of the tables in Towfic et al.
<br/>
<br/>
**Relationship to ["Gene expression analysis reveals functional pathways of glatiramer acetate activation."](http://informahealthcare.com/doi/abs/10.1517/14728222.2013.778829), Bakshi et al.**
The GEO deposit that Towfic et al. points to, but did not use (GSE40566), was used by another article by Bakshi et al. In that article, they do not count each sample twice and they do not use ComBat, still they find genes that differ between Copaxone and the generic. We have not made any attempt to reproduce the results in Bakshi et al., however based on their description, it seems that they used another batch adjustment tool, Partek Batch Remover™ tool, Partek Genomics Suite (Partek, Inc., St. Louis, Missouri, MO, USA) . In the relevant section of the manual for Partek Genomics Suite v6.6, it is described that the batch effect is estimated with a 2-way Anova and removed from the data, much in the same way as ComBat does (excluding the empirical Bayes step). The manual further emphasizes that the resulting data set is meant for visualization purposes and if used in a statistical test, the batch factor must be included. We obtained the manual from Partek user support, but have not acquired the permission to quote from it. From reading the methods section of Bakshi et al. it seems that the data set with batch effects removed was used in the statistical tests without supplying the batch as a factor, which may have lead to false results in a similar way as for Towfic et al.
<br/>
<br/>
For this report we will use GSE61901.
<br/>
<br/>

### Getting the data

loading libraries and helper script files needed.
```{r, results='hide', message=FALSE}
starttime = Sys.time()
debug = FALSE
downloaddata=TRUE
set.seed(100)

includelibs = c("sva", "limma", "xtable", "statmod", "GEOquery")
tmp=lapply(includelibs, require, character.only=T)
if(any(!unlist(tmp)))
{
  stop( paste("Not able to find all packages. Please install ",
              paste(includelibs[!unlist(tmp)], collapse=", ") )
              )
  #source("http://bioconductor.org/biocLite.R")
  #biocLite(includelibs)
}
source("../../commonscripts/helperfunctions.r")
rm(tmp)
```
<BR/>
<BR/>

The data used by Towfic et al. deposited in GEO with accession GSE61901. We need the sample annotation and non-normalized data. Be aware, that we are not using the GSE40566 here as is referred to in Towfic et al. and we used to generate the figure 3a in our manuscript.

```{r, message=FALSE}

geoaccession="GSE61901"

if(downloaddata)
{
	geoseries=getGEO(geoaccession)
	eset = geoseries[[1]]
}else{
	eset=getGEO(filename="not_in_github/GSE61901_series_matrix.txt.gz")
}
# will only use sample annotation from this eset. The data will be taken from the non-normalized matrix file.

# getting the sample annotation, some renaming for conveniance
duplicatesampleannotation = pData(eset)[, c("characteristics_ch1", "characteristics_ch1.3", "characteristics_ch1.2")]
names(duplicatesampleannotation) = c("batch", "covariate", "array_strip_address")
duplicatesampleannotation[,1]=gsub("batch: ", "", duplicatesampleannotation[,1] )
duplicatesampleannotation[,2]=gsub("batchcovar: ", "", duplicatesampleannotation[,2] )
duplicatesampleannotation[,3]=gsub("array_address: ", "", duplicatesampleannotation[,3] )
duplicatesampleannotation[,"array_hyb_address"]=gsub("_[12]", "", duplicatesampleannotation[,3] )

duplicatesampleannotation$covariate = make.names(duplicatesampleannotation$covariate)
duplicatesampleannotation$covariate[duplicatesampleannotation$covariate=="GA.DP"] = "DP"
duplicatesampleannotation$covariate[duplicatesampleannotation$covariate=="GA.Q"] = "N"
duplicatesampleannotation$covariate[duplicatesampleannotation$covariate=="Medium"] = "M"
duplicatesampleannotation$covariate[duplicatesampleannotation$covariate=="GA.RS"] = "RS"
rownames(duplicatesampleannotation) = duplicatesampleannotation[, "array_strip_address"]

# Creating a sample annotation for the data set where replicates are combined.
sampleannotation=duplicatesampleannotation[(1:(nrow(duplicatesampleannotation)/2))*2,c(1,2,4)]
rownames(sampleannotation) = sampleannotation[, "array_hyb_address"]

# Getting the measurments, i.e raw data.
# There are about 16000 empty coloumns, so this table is 30 times as big as it should be and needs 3GB in R 
# Currently takes about 8 minutes to read in.
if(downloaddata)
{
	temp = tempfile()
  download.file(url="http://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE61901&format=file&file=GSE61901%5Fnon%2Dnormalized%2Etxt%2Egz",
              destfile=temp, mode = "wb")
  orgrawtable = read.table(temp, sep="\t", header=TRUE, 
											stringsAsFactors=FALSE, skip=5, strip.white=TRUE, fill=TRUE)
  unlink(temp)
}else{
		orgrawtable = read.table("not_in_github/GSE61901_non-normalized.txt", sep="\t", header=TRUE, 
											stringsAsFactors=FALSE, skip=5, strip.white=TRUE, fill=TRUE)
}

#to matrix, taking out the empty detection p-val columns.
datamatrix_raw = orgrawtable
rawdataprobeids = datamatrix_raw[,1]
datamatrix_raw = datamatrix_raw[,(1:214)*2]
datamatrix_raw = as.matrix(datamatrix_raw)
mode(datamatrix_raw) = "numeric"
# table(is.na(datamatrix_raw))

#ordercheck
#table(colnames(datamatrix_raw)==paste( duplicatesampleannotation[, "array_strip_address"], sep="")) 
colnames(datamatrix_raw)=duplicatesampleannotation[, "array_strip_address"]
rownames(datamatrix_raw) = rawdataprobeids
rm(orgrawtable)
```

This is the top of the data matrix, illustrating the technical replication of samples.

```{r results="asis"}

print.xtable(xtable ( datamatrix_raw[1:5,1:6]), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)

```

The column headers have three parts, the BeadChip ID (“4634633002”), the hybridization id (“A”, 6 on each BeadChip) and the strip id (“1”, two for each hybridization).

<br/>
<br/>

The head of the sampleannotation table;

```{r results="asis"}

print.xtable(xtable ( head(sampleannotation), caption="Head of sampleannotation"), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)
```

The batch effect that was observed, was correlating to the BeadChip, thus the BeadChip identifier is used as batch label. Each BeadChip consists of 6 samples. The different treatments are used as covariate. The purpose of ComBat is to remove the batch-effect without removing the treatment effect.

<br/>

The naming convention seems to differ slightly between the Table S1 and the text in Towfic et al. and the GEO deposit. This is our naming convention of the main covariate labels and its corresponding name in the text of Towfic et al. and in GSE61901.

- **DP** referred to as "GA" (but not "GA" as in table S1), "GA.DP" in GEO
- **N** referred to as "generic", "GA.Q" in GEO
- **M** referred to as "medium"
- **RS** referred to as "reference standard", "GA.RS" in GEO

<BR/>
<BR/>

The batch/covariate design shows many batches and covariate groups.

```{r results='asis'}
print.xtable(xtable ( table(sampleannotation[, c("batch", "covariate")]), caption="Number of samples per batch/covariate combo. Chip barcode (batch) in rows, covariate in columns"), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)
```

<br/>
<br/>

A look at the primary comparison, "DP"(GA) and "N"(generic) reveals a lack of balance.

```{r results='asis'}

print.xtable(xtable ( table(sampleannotation[sampleannotation$covariate %in% c("DP", "N"),
										c("batch", "covariate")]),), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)
```

<br/>
<br/>



### Reproduce some of the original results

Towfic et al. performed several different tests for these data, and it is outside the scope of this report to reproduce all of their results. We focus on the key part of testing for differentially expressed genes between "GA" (DP) and "generic"(N) as described in Table S5. But prior to those tests, we have to preprocess the raw data. We will first use the data with two columns from each hybridization and do exactly as Towfic et al did, as described in the scripts provided (personal communication, Ben Zeskind).

```{r}
if(debug)
{
  datamatrix_raw = datamatrix_raw[1:1000,]
}
duplicatedatamatrix_qnorm = normalizeBetweenArrays(datamatrix_raw, method="quantile")
# normalize.quantiles from the package preprocessCore is used in the paper, but seems to do the same as the limma version.
```



```{r}
combatmod = model.matrix(~as.factor(duplicatesampleannotation$covariate))
duplicatedatamatrix_batchnorm= as.matrix(ComBat(dat=duplicatedatamatrix_qnorm,
                                                           batch=duplicatesampleannotation$batch,
                                                           mod=combatmod,
                                                           numCovs=NULL,
                                                           par.prior=TRUE,
                                                           prior.plots=FALSE))

```


<br/>
<br/>

**Table S5**

This is how the table looks. It is transposed and shows only the top 3 genes for visualization purposes. Be aware that a few of the rows will be skipped due to unparsable pdf-formatting.

```{r results="asis"}

# Table S5
table_s5 = read.table("data/table_s5.csv", sep="\t", header=TRUE, stringsAsFactors=FALSE, fill=TRUE)
tmp = as.matrix(table_s5)
tmp[,3:14] = as.numeric(tmp[,3:14])
# not able to paste the pdf whitout a lot of gibberish clutter the data. Some probes are lost!
a =rowSums(is.na(tmp[,3:14])) > 0
print(paste("Lost rows table s5: ", sum(a)))
table_s5=data.frame(table_s5[!a,], stringsAsFactors=FALSE)
table_s5$Fold_Change = as.numeric(table_s5$Fold_Change)
if(debug)
{  
  table_s5 = table_s5[table_s5$Probe %in% rownames(datamatrix_raw), ]
}
print.xtable(xtable (t(table_s5[1:3,]), caption="top of Table_S5"), 
			comment = TRUE,
      type = "html",
      html.table.attributes=c("CELLPADDING=3, BORDER=1"),
      include.rownames = TRUE)

```

<br/>
<br/>


We will reproduce the p-values from limma.

```{r dev='png', fig.width=10, fig.height=6, fig.show='hold', tidy=FALSE}
group = factor(duplicatesampleannotation$covariate)
design = model.matrix(~0 + group)
fit = lmFit(duplicatedatamatrix_batchnorm, design)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_ret_dup = eBayes(fit2)
limma_p_dup = limma_ret_dup$p.value
names(limma_p_dup) = names(limma_ret_dup$Amean)
plot(log10(table_s5[,"limma_p"]), log10(limma_p_dup[table_s5$Probe,1]), xlab="Original Table S5, log10", 
     ylab="Reproduced, log10", main="LIMMA P-values from table S5 vs. reproduced")

rm(group, design, fit,cont.matrix, fit2, limma_ret_dup, limma_p_dup)
```

The reproduced p-values for the Table S5 probes are exactly as the listed ones. This is not surprising, since the above code is only cosmetically different from the scripts used by Towfic et al. (personal communication, Ben Zeskind)

This concludes the exact reproduction. The rest of this report will show the difference in results from the analyses with or without ComBat, but otherwise conducted in a fairly standard way(in our opinion), i.e averaging the technical replicates.

**Limma test on average of techical replicates**

Combining the replicates after quantilenormalization and running ComBat.
```{r}

# Combine the two coloumns for each hyb.
# This was not perfored by Towfic et al.
datamatrix_qnorm = avearrays(duplicatedatamatrix_qnorm,  duplicatesampleannotation$array_hyb_address)
combatmod = model.matrix(~as.factor(sampleannotation$covariate))
datamatrix_batchnorm= as.matrix(ComBat(dat=datamatrix_qnorm,
                                                           batch=sampleannotation$batch,
                                                           mod=combatmod,
                                                           numCovs=NULL,
                                                           par.prior=TRUE,
                                                           prior.plots=FALSE))

# TEST!!! Se if it matters when to average duplicates. 
# datamatrix_batchnorm = avearrays(duplicatedatamatrix_batchnorm,  duplicatesampleannotation$array_hyb_address)
# It does. Somewhat smaller p-values than when averageing before combat.
```

Now, use limma on the ComBat-adjusted data.

```{r dev='png', fig.width=10, fig.height=6, fig.show='hold', tidy=FALSE}
group = factor(sampleannotation$covariate)
design = model.matrix(~0 + group)
fit = lmFit(datamatrix_batchnorm, design)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_ret_combat = eBayes(fit2)
limma_p_combat = limma_ret_combat$p.value
```

<br/>
<br/>

### Analysis without ComBat, and consequences for the results


The p-values in Table S5 could be strongly influenced by by ComBat's data transformation. An alternative, and preferable way in our opinion, of handling the batch effect is to include batch information in the statistical test. This can be done in limma in two ways. First what is described as "Blocking" in the limma user guide.

```{r dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}
group = factor(sampleannotation$covariate)
block = factor(sampleannotation$batch)
design = model.matrix(~0+group+block)
fit = lmFit(datamatrix_qnorm, design)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
#limma_ret_woc = eBayes(fit2)
#limma_p_woc = limma_ret_woc$p.value
#names(limma_p_woc) = names(limma_ret_woc$Amean)
limma_p_woc =  eBayes(fit2)$p.value
rm(design, group, block, fit, cont.matrix, fit2)
table(p.adjust(limma_p_combat , method="fdr")<0.05, p.adjust(limma_p_woc , method="fdr")<0.05, dnn=c("ComBat adj", "LIMMA adj"))
```

The number of significant probes using the given FDR threshold of 0.05 when batch effect is handled by LIMMA is 
**`r sum(p.adjust(limma_p_woc , method="fdr")<0.05)`**. When ComBat is applied, **`r sum(p.adjust(limma_p_combat , method="fdr")<0.05)`** are found.  The p-value distribution is still skewed, but not as much.

Alternatively, limma´s <i>duplicateCorrelation</i> function can be used to treat batch as a random effect. This could arguably be a more powerful approach, and a short investigation is warranted. Disclaimer, I am not 100% sure duplicateCorrelation is used correct below and as intended!

```{r dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}

group = factor(sampleannotation$covariate)
block = factor(sampleannotation$batch)
design = model.matrix(~0+group)

corfit <- duplicateCorrelation(datamatrix_qnorm,design,block=block)
fit <- lmFit(datamatrix_qnorm,design,block=block,correlation=corfit$consensus)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
fit2 = contrasts.fit(fit, cont.matrix)
#limma_ret_woc = eBayes(fit2)
limma_p_mixed = eBayes(fit2)$p.value
thiscolors = c("red", "blue", "black")
hist(limma_p_combat, breaks=100, freq=T, border=thiscolors[1], main="Alternative P-value distributions for DP vs. N", xlab="P-values")
hist(limma_p_woc, breaks=100, add=T, freq=T, border=thiscolors[2])
hist(limma_p_mixed, breaks=100, add=T, freq=T, border=thiscolors[3])
legend("topright", legend=c("ComBat adjusted", "batch handled by LIMMA as factor", "batch handled by LIMMA as random effect"), text.col=thiscolors)
```

For this data set, the two LIMMA methods will give quite similar results, at least compared to the ComBat adjusted approach. We will only use the first, "Blocking", in subsequent comparisons.

<br/>
<br/>

### Additional sanity checks

To substantiate that the result from the use of ComBat is less trustworthy than the alternative analysis, we provide a couple of additional sanity checks.

<br/>
<br/>

**Random data**

First, we use random numbers drawn from the same distribution(mean=0, sd=1) regardless of batch or covariate, while retaining the batch/covariate design.

```{r}
set.seed(100)
datamatrix_randraw = matrix(rnorm(length(datamatrix_qnorm), mean=0, sd=1), 
                                      nrow=nrow(datamatrix_qnorm), 
                                      ncol=ncol(datamatrix_qnorm), 
                                      dimnames=dimnames(datamatrix_qnorm))
combatmod = model.matrix(~as.factor(sampleannotation$covariate))
datamatrix_randcombat= as.matrix(ComBat(dat=datamatrix_randraw,
                                                             batch=sampleannotation$batch, 
                                                             mod=combatmod, 
                                                             numCovs=NULL, 
                                                             par.prior=TRUE, 
                                                             prior.plots=FALSE))
```

limma is then used in 3 ways

- On the ComBat adjusted random numbers
- On the random numbers ignoring batch information
- On the random numbers including batch as a blocking factor


```{r}
group = factor(sampleannotation$covariate)
design = model.matrix(~0 + group)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)


fit = lmFit(datamatrix_randcombat, design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_p_rand_combat = eBayes(fit2)$p.value[,1]

fit = lmFit(datamatrix_randraw, design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_p_rand_nocombat = eBayes(fit2)$p.value[,1]


block = as.factor(sampleannotation$batch)
design = model.matrix(~0+group+block)
cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)
fit = lmFit(datamatrix_randraw, design)  
fit2 = contrasts.fit(fit, cont.matrix)
limma_p_rand_batchblocked = eBayes(fit2)$p.value[,1]

```

```{r dev='png', fig.width=8, fig.height=8, fig.show='hold', tidy=FALSE}
par(mfrow=c(1, 1))
thiscolors = c("red", "blue", "black")
hist(limma_p_rand_combat, breaks=100, freq=T, border=thiscolors[1], main="P-values, Random numbers for DP vs. N")
hist(limma_p_rand_nocombat, breaks=100, add=T, freq=T, border=thiscolors[2])
hist(limma_p_rand_batchblocked, breaks=100, add=T, freq=T, border=thiscolors[3])
legend("topright", legend=c("ComBat adjusted", "batch handled by Limma as factor", "No adjustment"), text.col=thiscolors)
```

The p-value plots from the ComBat adjusted random data shows a skewed distribution, although not as much as for the real data. The random data has no batch effect or other effects like the real data which might affect the ComBat adjustment.


<br/>
<br/>

**Permuted labels**

Another sanity check is by permuting the DP (GA) and N(generic) labels and observe changes in the results. The real data are used, and in order to retain the batch effect, we will only swap labels within a batch. This should in theory give fewer significant probes.

```{r results='hide'}
source("../../commonscripts/helperfunctions.r")

nshuffleddatasets = 10
datamatrices_permuted = list()
changedlabels=vector()
combatmod = model.matrix(~as.factor(sampleannotation$covariate))
for(i in 1:nshuffleddatasets)
{
  x = shufflesamplesinbatch( rownames(sampleannotation), sampleannotation$batch, sampleannotation$covariate, c("N", "DP") )
  changedlabels[i] = (  sum(sampleannotation[,"covariate"] != sampleannotation[x,"covariate"]  ))  # see how many got change. Has 11 N's)  
  datamatrices_permuted[[i]] = as.matrix(ComBat(dat=datamatrix_qnorm[, x], 
                                                batch=sampleannotation$batch, 
                                                mod=combatmod, 
                                                numCovs=NULL, 
                                                par.prior=TRUE, 
                                                prior.plots=FALSE))
}
```

**`r nshuffleddatasets`** permutations were created and batch adjusted with ComBat.

Then perform the limma test on the ComBat adjusted data as was done for the real data.

```{r}
permuted_pvalcounts=list()
overviewtab = data.frame(permutation=1:nshuffleddatasets, changedlabels=changedlabels, significantprobes=NA)
for(i in 1:nshuffleddatasets)
{ 
  group = factor(sampleannotation$covariate)
  design = model.matrix(~0 + group)
  fit = lmFit(datamatrices_permuted[[i]], design)
  cont.matrix = makeContrasts ( contrasts="groupDP-groupN", levels=design)  
  fit2 = contrasts.fit(fit, cont.matrix)  
  this_p = eBayes(fit2)$p.value
  permuted_pvalcounts[[i]] = hist(this_p, plot=FALSE, breaks=100)$counts 
  overviewtab$significantprobes[i] =sum(p.adjust(this_p, method="fdr")<0.05)
}
print(overviewtab)
rm(overviewtab, group, design, fit, cont.matrix, fit2, this_p)
```

The table lists the number of labels that are changed compared to the real ones for each permutation. Remember that the total number of N (generic) samples is **`r sum(sampleannotation$covariate=="N")`**. So even when about half of those are swapped with DP(GA), the ComBat adjusted data produces many significant genes. The number of significant genes using the same 0.05 FDR cut-off for the real labels was **`r sum(p.adjust(limma_p_combat , method="fdr")<0.05)`**.

The P-value distribution can also be plotted.

```{r, dev='svg', fig.width=8, fig.height=8, tidy=FALSE}
real_pvalcounts=hist(limma_p_combat, plot=FALSE, breaks=100)$counts
plot((1:20)/100, real_pvalcounts[1:20], col="red",
     main="P-values, permuted real data", 
      xlab="p-value", ylab="Frequency", type="l", lwd=2, 
      ylim=c(0, max(unlist( c(permuted_pvalcounts,real_pvalcounts)))))
for(i in 1:length(permuted_pvalcounts))
{
  lines((1:20)/100,permuted_pvalcounts[[i]][1:20], col="blue")
}
legend("topright", legend=c("Reproduced p-values real labels", "Permuted DP and N labels"), text.col=c("red", "blue"))
```

Shuffling the N and DP labels within batches produces about the same p-value distribution as for the real data. This is an indication that the significant genes found in the real data are not altered by the treatment, but rather an adverse effect of the ComBat adjustments.


<br/>
<br/>

**Control probes found as differentially expressed**

[Table S5](http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0083757.s013) from Towfic et al. lists the probes with the most average change in expressed between DP(GA) and N(generic) alongside the results from several test for differential expression. Two of the probes listed and found to be differentially expressed are control probes, (ILMN_1343049 and ILMN_1343048). These are not targeting mouse RNA, but instead target added alien RNA of a known concentration, and are intended to be used in quality control of the hybridization. There are 12 control probes of this type on the array. These two probes may have gotten such low p-values by chance, but in our opinion the more likely explanation is that something has gone wrong in the analysis steps. The use of ComBat is one possible explanation since it is able to find differentially expressed genes also for random numbers.


<br/>
<br/>

### References

Towfic F, Funt JM, Fowler KD, Bakshi S, Blaugrund E, Artyomov MN, Hayden MR,
Ladkani D, Schwartz R, Zeskind B. Comparing the biological impact of glatiramer
acetate with the biological impact of a generic. PLoS One. 2014 Jan
8;9(1):e83757. doi: 10.1371/journal.pone.0083757. eCollection 2014. PubMed PMID: 
24421904; PubMed Central PMCID: PMC3885444.


Johnson, WE, Rabinovic, A, and Li, C (2007). Adjusting batch effects in microarray expression data using Empirical Bayes methods. Biostatistics 8(1):118-127.

Leek JT, Johnson WE, Parker HS, Jaffe AE, Storey JD.(2012) The sva package for removing batch effects and other unwanted variation in high-throughput experiments. Bioinformatics. 2012 Mar 15;28(6):882-3.

Smyth, GK (2005). Limma: linear models for microarray data. In: 'Bioinformatics and Computational Biology Solutions
  using R and Bioconductor'. R. Gentleman, V. Carey, S. Dudoit, R. Irizarry, W. Huber (eds), Springer, New York, pages
  397-420.
  
  R Core Team (2013). R: A language and environment for statistical computing. R Foundation for Statistical Computing,
  Vienna, Austria. URL http://www.R-project.org/

  Yihui Xie (2013). knitr: A general-purpose package for dynamic report generation in R. R package version 1.5.

  Yihui Xie (2013) Dynamic Documents with R and knitr. Chapman and Hall/CRC. ISBN 978-1482203530

  Yihui Xie (2013) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and
  Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595
  
  RStudio Team (2012). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA URL http://www.rstudio.com/.


```{r sessionInfo, comment=""}
sessionInfo()
```

generation ended `r as.character(Sys.time())`. Time spent `r  as.integer(round(difftime(Sys.time(),starttime, units="mins")))` minutes .


